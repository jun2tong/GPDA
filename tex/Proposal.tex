%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CSCI 1430 Project Progress Report Template
%
% This is a LaTeX document. LaTeX is a markup language for producing documents.
% Your task is to answer the questions by filling out this document, then to 
% compile this into a PDF document. 
% You will then upload this PDF to `Gradescope' - the grading system that we will use. 
% Instructions for upload will follow soon.
%
% 
% TO COMPILE:
% > pdflatex thisfile.tex
%
% If you do not have LaTeX and need a LaTeX distribution:
% - Departmental machines have one installed.
% - Personal laptops (all common OS): http://www.latex-project.org/get/
%
% If you need help with LaTeX, come to office hours. Or, there is plenty of help online:
% https://en.wikibooks.org/wiki/LaTeX
%
% Good luck!
% James and the 1430 staff
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% How to include two graphics on the same line:
% 
% \includegraphics[width=0.49\linewidth]{yourgraphic1.png}
% \includegraphics[width=0.49\linewidth]{yourgraphic2.png}
%
% How to include equations:
%
% \begin{equation}
% y = mx+c
% \end{equation}
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue]{hyperref}
\usepackage[a4paper,margin=1.5in]{geometry}
\usepackage{stackengine,graphicx}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\usepackage{microtype}
\usepackage{times}
\usepackage{booktabs}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
% From https://ctan.org/pkg/matlab-prettifier
\usepackage[numbered,framed]{matlab-prettifier}

\frenchspacing
\setlength{\parindent}{0cm} % Default is 15pt.
\setlength{\parskip}{0.3cm plus1mm minus1mm}

\pagestyle{fancy}
\fancyhf{}
\lhead{Final Project Proposal}
\rhead{SYDE 671}
\rfoot{\thepage}

\date{}

\title{\vspace{-1cm}Final Project Proposal}


\begin{document}
\maketitle
% \vspace{-0cm}
\thispagestyle{fancy}

\textbf{Team name: MaxMarginDB}

\textbf{Team members: JunYong Tong, Nick Torenvliet}

\section*{Project}
The selected paper is \textbf{Unsupervised Visual Domain Adaptation: A Deep Max-Margin Approach} ~\cite{kim2019unsupervised} from CVPR 2019.

The codebase provided with the paper is implemented in the PyTorch framework using the MNIST, SVHN and USPS datasets. 


% Ask for help on practical application of the problem.

\section*{Problem Setup/Statement}
Consider a joint space of inputs and class labels, \(\mathcal{X} \times \mathcal{Y}\) where \(\mathcal{Y} = \{1,\dots,K\}\). 
Suppose we have two domains on this space, the \textbf{source (S)} and the \textbf{target (T)}, defined by unknown distributions \(p_S(\vect{x}, y)\) and \(p_T(\vect{x},y)\).
We are given source-domain training examples with labels \(\mathcal{D}_S = \{\vect{x}_i^S, y_i\}_{i=1}^{N_S}\) and target data \(\mathcal{D}_T = \{\vect{x}_I^T\}_{i=1}^{N_T}\) without labels.
Assuming a \textbf{shared set of class labels} between the two domains. The goal is to assign the correct class labels to target data points.

The state of the art (SOTA) on this problem is the maximum classifier discrepancy algorithm (MCDA)~\cite{saito2018maximum}. MCDA involves training two neural networks using adversarial-cooperative training to minimize,
\begin{equation} \label{eq:minimax}
    \sup_{h_1, h_2 \in \mathcal{H}} \mathbb{E}_{\vect{x} \sim p_T} \, [\mathbb{I}(h_1(\vect{x}) \neq h_2(\vect{x}))]
\end{equation}
where \(\mathcal{H}\) is the set of classifiers that has small errors on \(\vect{S}\), \(h_1, h_2\) are two deep neural net classifiers from \(\mathcal{H}\), and \(\mathbb{I}(\cdot)\) is the indicator function. This would minimize a theoretical upper bound of the error for this class of problem~\cite{ben2010theory}, thus minimizing the error on \(\vect{T}\).

\section*{Their Contribution Overview}
The method advanced in \textbf{Unsupervised Visual Domain Adaptation: A Deep Max-Margin Approach} ~\cite{kim2019unsupervised} reduces the computational resources required to achieve SOTA results and provides a max-margin argument within the computation. 

The novel approach is to adopt a Bayesian framework to induce hypothesis space \(\mathcal{H}\). This is achieved by building a Gaussian Process (GP) classifier model on top of the shared feature map which naturally defines \(\mathcal{H}\) the posterior, and then optimizing the shared space and the GP kernel to reduce Equation (\ref{eq:minimax}). 

This method requires only one trained network classifier, as opposed to the two required by MCDA, and also introduces the concept of max-margin within the framework.  

\section*{Questions to be Asked}
\begin{itemize}
    \item Could we do better than GP? Try a general distribution from exponential family.
    \item Due to non-linearity in the feature map, a normal i.i.d. variational technique was used to estimate the GP posterior. One improvement might be to use a black-box variational inference
    ~\cite{ranganath2014black}, instead of a normal, on the weights for the GP classifier.
    \item In the derivation of the evidence lower bound for optimization (ELBO), the intractable log-likelihood term was estimated using MCMC. We may improve the estimate by adopting an alternate estimation technique.
\end{itemize}

\section*{Plan of Attack}
\begin{itemize}
    \item Week of November 18: get the code running, reproduce results in the paper, finalize ideas for improvements
    \item Week of November 25: implement idea for improvements
    \item Week of December 2: iterate implementation
    \item Week of December 9: Report writing
\end{itemize}
% \section*{Remarks}
% Their problem is fundamentally different from our catastrophic forgetting problem because at no point in time, we have data from both task 1 and task 2.

% The similarities lie in the works/ideas that we are both trying separate the classes using some large-margin techniques and through separating (normal) distribution. 
% This work ~\cite{kim2019unsupervised} suggests that our idea of modelling each class as Gaussian is on the right track.
\bibliographystyle{ieee}
\bibliography{references}


\end{document}